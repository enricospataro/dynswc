1
00:00:12,820 --> 00:00:15,820
Suppose that two American friends are traveling together in Italy.

2
00:00:15,820 --> 00:00:17,820
They go to see Michelangelo's "David,"

3
00:00:17,820 --> 00:00:19,820
and when they finally come face to face with the statue,

4
00:00:19,820 --> 00:00:21,820
they both freeze dead in their tracks.

5
00:00:21,820 --> 00:00:23,820
The first guy -- we'll call him Adam --

6
00:00:23,820 --> 00:00:26,820
is transfixed by the beauty of the perfect human form.

7
00:00:26,820 --> 00:00:28,820
The second guy -- we'll call him Bill --

8
00:00:28,820 --> 00:00:32,820
is transfixed by embarrassment, at staring at the thing there in the center.

9
00:00:33,820 --> 00:00:35,820
So here's my question for you:

10
00:00:35,820 --> 00:00:39,820
which one of these two guys was more likely to have voted for George Bush,

11
00:00:39,820 --> 00:00:41,820
which for Al Gore?

12
00:00:41,820 --> 00:00:42,820
I don't need a show of hands

13
00:00:42,820 --> 00:00:45,820
because we all have the same political stereotypes.

14
00:00:45,820 --> 00:00:47,820
We all know that it's Bill.

15
00:00:47,820 --> 00:00:51,820
And in this case, the stereotype corresponds to reality.

16
00:00:51,820 --> 00:00:54,820
It really is a fact that liberals are much higher than conservatives

17
00:00:54,820 --> 00:00:57,820
on a major personality trait called openness to experience.

18
00:00:57,820 --> 00:00:59,820
People who are high in openness to experience

19
00:00:59,820 --> 00:01:03,820
just crave novelty, variety, diversity, new ideas, travel.

20
00:01:03,820 --> 00:01:08,820
People low on it like things that are familiar, that are safe and dependable.

21
00:01:08,820 --> 00:01:10,820
If you know about this trait,

22
00:01:10,820 --> 00:01:12,820
you can understand a lot of puzzles about human behavior.

23
00:01:12,820 --> 00:01:15,820
You can understand why artists are so different from accountants.

24
00:01:15,820 --> 00:01:17,820
You can actually predict what kinds of books they like to read,

25
00:01:17,820 --> 00:01:19,820
what kinds of places they like to travel to,

26
00:01:19,820 --> 00:01:21,820
and what kinds of food they like to eat.

27
00:01:21,820 --> 00:01:24,820
Once you understand this trait, you can understand

28
00:01:24,820 --> 00:01:28,820
why anybody would eat at Applebee's, but not anybody that you know.

29
00:01:28,820 --> 00:01:34,820
(Laughter)

30
00:01:34,820 --> 00:01:36,820
This trait also tells us a lot about politics.

31
00:01:36,820 --> 00:01:39,820
The main researcher of this trait, Robert McCrae says that,

32
00:01:39,820 --> 00:01:43,820
"Open individuals have an affinity for liberal, progressive, left-wing political views" --

33
00:01:43,820 --> 00:01:45,820
they like a society which is open and changing --

34
00:01:45,820 --> 00:01:50,820
"whereas closed individuals prefer conservative, traditional, right-wing views."

35
00:01:50,820 --> 00:01:54,820
This trait also tells us a lot about the kinds of groups people join.

36
00:01:54,820 --> 00:01:56,820
So here's the description of a group I found on the Web.

37
00:01:56,820 --> 00:01:58,820
What kinds of people would join a global community

38
00:01:58,820 --> 00:02:00,820
welcoming people from every discipline and culture,

39
00:02:00,820 --> 00:02:02,820
who seek a deeper understanding of the world,

40
00:02:02,820 --> 00:02:05,820
and who hope to turn that understanding into a better future for us all?

41
00:02:05,820 --> 00:02:07,820
This is from some guy named Ted.

42
00:02:07,820 --> 00:02:09,820
(Laughter)

43
00:02:09,820 --> 00:02:13,820
Well, let's see now, if openness predicts who becomes liberal,

44
00:02:13,820 --> 00:02:15,820
and openness predicts who becomes a TEDster,

45
00:02:15,820 --> 00:02:18,820
then might we predict that most TEDsters are liberal?

46
00:02:18,820 --> 00:02:19,820
Let's find out.

47
00:02:19,820 --> 00:02:23,820
I'm going to ask you to raise your hand, whether you are liberal, left of center --

48
00:02:23,820 --> 00:02:25,820
on social issues, we're talking about, primarily --

49
00:02:25,820 --> 00:02:27,820
or conservative, and I'll give a third option,

50
00:02:27,820 --> 00:02:29,820
because I know there are a number of libertarians in the audience.

51
00:02:29,820 --> 00:02:31,820
So, right now, please raise your hand --

52
00:02:31,820 --> 00:02:32,820
down in the simulcast rooms, too,

53
00:02:32,820 --> 00:02:34,820
let's let everybody see who's here --

54
00:02:34,820 --> 00:02:37,820
please raise your hand if you would say that you are liberal or left of center.

55
00:02:37,820 --> 00:02:40,820
Please raise your hand high right now. OK.

56
00:02:41,820 --> 00:02:43,820
Please raise your hand if you'd say you're libertarian.

57
00:02:44,820 --> 00:02:46,820
OK, about a -- two dozen.

58
00:02:46,820 --> 00:02:49,820
And please raise your hand if you'd say you are right of center or conservative.

59
00:02:49,820 --> 00:02:54,820
One, two, three, four, five -- about eight or 10.

60
00:02:55,820 --> 00:02:58,820
OK. This is a bit of a problem.

61
00:02:58,820 --> 00:03:01,820
Because if our goal is to understand the world,

62
00:03:01,820 --> 00:03:03,820
to seek a deeper understanding of the world,

63
00:03:03,820 --> 00:03:06,820
our general lack of moral diversity here is going to make it harder.

64
00:03:06,820 --> 00:03:10,820
Because when people all share values, when people all share morals,

65
00:03:10,820 --> 00:03:13,820
they become a team, and once you engage the psychology of teams,

66
00:03:13,820 --> 00:03:15,820
it shuts down open-minded thinking.

67
00:03:18,820 --> 00:03:22,820
When the liberal team loses, as it did in 2004,

68
00:03:22,820 --> 00:03:26,820
and as it almost did in 2000, we comfort ourselves.

69
00:03:26,820 --> 00:03:28,820
(Laughter)

70
00:03:28,820 --> 00:03:32,820
We try to explain why half of America voted for the other team.

71
00:03:32,820 --> 00:03:37,820
We think they must be blinded by religion, or by simple stupidity.

72
00:03:37,820 --> 00:03:40,820
(Laughter)

73
00:03:40,820 --> 00:03:48,820
(Applause)

74
00:03:48,820 --> 00:03:54,820
So, if you think that half of America votes Republican

75
00:03:54,820 --> 00:03:57,820
because they are blinded in this way,

76
00:03:57,820 --> 00:04:00,820
then my message to you is that you're trapped in a moral matrix,

77
00:04:00,820 --> 00:04:01,820
in a particular moral matrix.

78
00:04:01,820 --> 00:04:05,820
And by the matrix, I mean literally the matrix, like the movie "The Matrix."

79
00:04:05,820 --> 00:04:07,820
But I'm here today to give you a choice.

80
00:04:07,820 --> 00:04:11,820
You can either take the blue pill and stick to your comforting delusions,

81
00:04:11,820 --> 00:04:13,820
or you can take the red pill,

82
00:04:13,820 --> 00:04:16,820
learn some moral psychology and step outside the moral matrix.

83
00:04:16,820 --> 00:04:18,820
Now, because I know --

84
00:04:18,820 --> 00:04:21,820
(Applause) --

85
00:04:21,820 --> 00:04:23,820
OK, I assume that answers my question.

86
00:04:23,820 --> 00:04:25,820
I was going to ask you which one you picked, but no need.

87
00:04:25,820 --> 00:04:27,820
You're all high in openness to experience, and besides,

88
00:04:27,820 --> 00:04:30,820
it looks like it might even taste good, and you're all epicures.

89
00:04:30,820 --> 00:04:32,820
So anyway, let's go with the red pill.

90
00:04:32,820 --> 00:04:34,820
Let's study some moral psychology and see where it takes us.

91
00:04:34,820 --> 00:04:36,820
Let's start at the beginning.

92
00:04:36,820 --> 00:04:38,820
What is morality and where does it come from?

93
00:04:38,820 --> 00:04:40,820
The worst idea in all of psychology

94
00:04:40,820 --> 00:04:43,820
is the idea that the mind is a blank slate at birth.

95
00:04:43,820 --> 00:04:45,820
Developmental psychology has shown

96
00:04:45,820 --> 00:04:47,820
that kids come into the world already knowing so much

97
00:04:47,820 --> 00:04:49,820
about the physical and social worlds,

98
00:04:49,820 --> 00:04:53,820
and programmed to make it really easy for them to learn certain things

99
00:04:53,820 --> 00:04:54,820
and hard to learn others.

100
00:04:54,820 --> 00:04:56,820
The best definition of innateness I've ever seen --

101
00:04:56,820 --> 00:04:58,820
this just clarifies so many things for me --

102
00:04:58,820 --> 00:05:00,820
is from the brain scientist Gary Marcus.

103
00:05:00,820 --> 00:05:05,820
He says, "The initial organization of the brain does not depend that much on experience.

104
00:05:05,820 --> 00:05:08,820
Nature provides a first draft, which experience then revises.

105
00:05:08,820 --> 00:05:10,820
Built-in doesn't mean unmalleable;

106
00:05:10,820 --> 00:05:13,820
it means organized in advance of experience."

107
00:05:13,820 --> 00:05:15,820
OK, so what's on the first draft of the moral mind?

108
00:05:15,820 --> 00:05:18,820
To find out, my colleague, Craig Joseph, and I

109
00:05:18,820 --> 00:05:20,820
read through the literature on anthropology,

110
00:05:20,820 --> 00:05:22,820
on culture variation in morality

111
00:05:22,820 --> 00:05:24,820
and also on evolutionary psychology, looking for matches.

112
00:05:24,820 --> 00:05:27,820
What are the sorts of things that people talk about across disciplines?

113
00:05:27,820 --> 00:05:29,820
That you find across cultures and even across species?

114
00:05:29,820 --> 00:05:31,820
We found five -- five best matches,

115
00:05:31,820 --> 00:05:33,820
which we call the five foundations of morality.

116
00:05:33,820 --> 00:05:35,820
The first one is harm/care.

117
00:05:35,820 --> 00:05:39,820
We're all mammals here, we all have a lot of neural and hormonal programming

118
00:05:39,820 --> 00:05:41,820
that makes us really bond with others, care for others,

119
00:05:41,820 --> 00:05:44,820
feel compassion for others, especially the weak and vulnerable.

120
00:05:44,820 --> 00:05:47,820
It gives us very strong feelings about those who cause harm.

121
00:05:47,820 --> 00:05:50,820
This moral foundation underlies about 70 percent

122
00:05:50,820 --> 00:05:52,820
of the moral statements I've heard here at TED.

123
00:05:52,820 --> 00:05:55,820
The second foundation is fairness/reciprocity.

124
00:05:55,820 --> 00:05:57,820
There's actually ambiguous evidence

125
00:05:57,820 --> 00:05:59,820
as to whether you find reciprocity in other animals,

126
00:05:59,820 --> 00:06:01,820
but the evidence for people could not be clearer.

127
00:06:01,820 --> 00:06:03,820
This Norman Rockwell painting is called "The Golden Rule,"

128
00:06:03,820 --> 00:06:05,820
and we heard about this from Karen Armstrong, of course,

129
00:06:05,820 --> 00:06:08,820
as the foundation of so many religions.

130
00:06:08,820 --> 00:06:10,820
That second foundation underlies the other 30 percent

131
00:06:10,820 --> 00:06:12,820
of the moral statements I've heard here at TED.

132
00:06:12,820 --> 00:06:14,820
The third foundation is in-group/loyalty.

133
00:06:14,820 --> 00:06:16,820
You do find groups in the animal kingdom --

134
00:06:16,820 --> 00:06:18,820
you do find cooperative groups --

135
00:06:18,820 --> 00:06:21,820
but these groups are always either very small or they're all siblings.

136
00:06:21,820 --> 00:06:24,820
It's only among humans that you find very large groups of people

137
00:06:24,820 --> 00:06:27,820
who are able to cooperate, join together into groups,

138
00:06:27,820 --> 00:06:31,820
but in this case, groups that are united to fight other groups.

139
00:06:31,820 --> 00:06:35,820
This probably comes from our long history of tribal living, of tribal psychology.

140
00:06:35,820 --> 00:06:37,820
And this tribal psychology is so deeply pleasurable

141
00:06:37,820 --> 00:06:39,820
that even when we don't have tribes,

142
00:06:39,820 --> 00:06:42,820
we go ahead and make them, because it's fun.

143
00:06:42,820 --> 00:06:45,820
(Laughter)

144
00:06:45,820 --> 00:06:48,820
Sports is to war as pornography is to sex.

145
00:06:48,820 --> 00:06:51,820
We get to exercise some ancient, ancient drives.

146
00:06:51,820 --> 00:06:54,820
The fourth foundation is authority/respect.

147
00:06:54,820 --> 00:06:57,820
Here you see submissive gestures from two members of very closely related species.

148
00:06:57,820 --> 00:07:01,820
But authority in humans is not so closely based on power and brutality,

149
00:07:01,820 --> 00:07:03,820
as it is in other primates.

150
00:07:03,820 --> 00:07:05,820
It's based on more voluntary deference,

151
00:07:05,820 --> 00:07:07,820
and even elements of love, at times.

152
00:07:07,820 --> 00:07:09,820
The fifth foundation is purity/sanctity.

153
00:07:09,820 --> 00:07:12,820
This painting is called "The Allegory Of Chastity,"

154
00:07:12,820 --> 00:07:15,820
but purity's not just about suppressing female sexuality.

155
00:07:15,820 --> 00:07:18,820
It's about any kind of ideology, any kind of idea

156
00:07:18,820 --> 00:07:20,820
that tells you that you can attain virtue

157
00:07:20,820 --> 00:07:21,820
by controlling what you do with your body,

158
00:07:21,820 --> 00:07:23,820
by controlling what you put into your body.

159
00:07:23,820 --> 00:07:27,820
And while the political right may moralize sex much more,

160
00:07:27,820 --> 00:07:29,820
the political left is really doing a lot of it with food.

161
00:07:29,820 --> 00:07:31,820
Food is becoming extremely moralized nowadays,

162
00:07:31,820 --> 00:07:33,820
and a lot of it is ideas about purity,

163
00:07:33,820 --> 00:07:36,820
about what you're willing to touch, or put into your body.

164
00:07:36,820 --> 00:07:39,820
I believe these are the five best candidates

165
00:07:39,820 --> 00:07:41,820
for what's written on the first draft of the moral mind.

166
00:07:41,820 --> 00:07:42,820
I think this is what we come with, at least

167
00:07:42,820 --> 00:07:45,820
a preparedness to learn all of these things.

168
00:07:45,820 --> 00:07:48,820
But as my son, Max, grows up in a liberal college town,

169
00:07:49,820 --> 00:07:51,820
how is this first draft going to get revised?

170
00:07:51,820 --> 00:07:53,820
And how will it end up being different

171
00:07:53,820 --> 00:07:56,820
from a kid born 60 miles south of us in Lynchburg, Virginia?

172
00:07:56,820 --> 00:07:58,820
To think about culture variation, let's try a different metaphor.

173
00:07:58,820 --> 00:08:01,820
If there really are five systems at work in the mind --

174
00:08:01,820 --> 00:08:03,820
five sources of intuitions and emotions --

175
00:08:03,820 --> 00:08:05,820
then we can think of the moral mind

176
00:08:05,820 --> 00:08:07,820
as being like one of those audio equalizers that has five channels,

177
00:08:07,820 --> 00:08:09,820
where you can set it to a different setting on every channel.

178
00:08:09,820 --> 00:08:12,820
And my colleagues, Brian Nosek and Jesse Graham, and I,

179
00:08:12,820 --> 00:08:17,820
made a questionnaire, which we put up on the Web at www.YourMorals.org.

180
00:08:17,820 --> 00:08:22,820
And so far, 30,000 people have taken this questionnaire, and you can too.

181
00:08:22,820 --> 00:08:23,820
Here are the results.

182
00:08:23,820 --> 00:08:26,820
Here are the results from about 23,000 American citizens.

183
00:08:26,820 --> 00:08:28,820
On the left, I've plotted the scores for liberals;

184
00:08:28,820 --> 00:08:30,820
on the right, those for conservatives; in the middle, the moderates.

185
00:08:30,820 --> 00:08:32,820
The blue line shows you people's responses

186
00:08:32,820 --> 00:08:34,820
on the average of all the harm questions.

187
00:08:34,820 --> 00:08:37,820
So, as you see, people care about harm and care issues.

188
00:08:37,820 --> 00:08:39,820
They give high endorsement of these sorts of statements

189
00:08:39,820 --> 00:08:41,820
all across the board, but as you also see,

190
00:08:41,820 --> 00:08:44,820
liberals care about it a little more than conservatives -- the line slopes down.

191
00:08:44,820 --> 00:08:46,820
Same story for fairness.

192
00:08:46,820 --> 00:08:48,820
But look at the other three lines.

193
00:08:48,820 --> 00:08:50,820
For liberals, the scores are very low.

194
00:08:50,820 --> 00:08:52,820
Liberals are basically saying, "No, this is not morality.

195
00:08:52,820 --> 00:08:55,820
In-group, authority, purity -- this stuff has nothing to do with morality. I reject it."

196
00:08:55,820 --> 00:08:57,820
But as people get more conservative, the values rise.

197
00:08:57,820 --> 00:09:00,820
We can say that liberals have a kind of a two-channel,

198
00:09:00,820 --> 00:09:01,820
or two-foundation morality.

199
00:09:01,820 --> 00:09:03,820
Conservatives have more of a five-foundation,

200
00:09:03,820 --> 00:09:05,820
or five-channel morality.

201
00:09:05,820 --> 00:09:06,820
We find this in every country we look at.

202
00:09:06,820 --> 00:09:08,820
Here's the data for 1,100 Canadians.

203
00:09:08,820 --> 00:09:10,820
I'll just flip through a few other slides.

204
00:09:10,820 --> 00:09:13,820
The U.K., Australia, New Zealand, Western Europe, Eastern Europe,

205
00:09:13,820 --> 00:09:17,820
Latin America, the Middle East, East Asia and South Asia.

206
00:09:17,820 --> 00:09:19,820
Notice also that on all of these graphs,

207
00:09:19,820 --> 00:09:22,820
the slope is steeper on in-group, authority, purity.

208
00:09:22,820 --> 00:09:24,820
Which shows that within any country,

209
00:09:24,820 --> 00:09:27,820
the disagreement isn't over harm and fairness.

210
00:09:27,820 --> 00:09:29,820
Everybody -- I mean, we debate over what's fair --

211
00:09:29,820 --> 00:09:32,820
but everybody agrees that harm and fairness matter.

212
00:09:32,820 --> 00:09:34,820
Moral arguments within cultures

213
00:09:34,820 --> 00:09:37,820
are especially about issues of in-group, authority, purity.

214
00:09:37,820 --> 00:09:40,820
This effect is so robust that we find it no matter how we ask the question.

215
00:09:40,820 --> 00:09:42,820
In one recent study,

216
00:09:42,820 --> 00:09:44,820
we asked people to suppose you're about to get a dog.

217
00:09:44,820 --> 00:09:45,820
You picked a particular breed,

218
00:09:45,820 --> 00:09:47,820
you learned some new information about the breed.

219
00:09:47,820 --> 00:09:50,820
Suppose you learn that this particular breed is independent-minded,

220
00:09:50,820 --> 00:09:52,820
and relates to its owner as a friend and an equal?

221
00:09:52,820 --> 00:09:54,820
Well, if you are a liberal, you say, "Hey, that's great!"

222
00:09:54,820 --> 00:09:56,820
Because liberals like to say, "Fetch, please."

223
00:09:56,820 --> 00:10:00,820
(Laughter)

224
00:10:01,820 --> 00:10:04,820
But if you're conservative, that's not so attractive.

225
00:10:04,820 --> 00:10:07,820
If you're conservative, and you learn that a dog's extremely loyal

226
00:10:07,820 --> 00:10:09,820
to its home and family, and doesn't warm up quickly to strangers,

227
00:10:09,820 --> 00:10:12,820
for conservatives, well, loyalty is good -- dogs ought to be loyal.

228
00:10:12,820 --> 00:10:14,820
But to a liberal, it sounds like this dog

229
00:10:14,820 --> 00:10:16,820
is running for the Republican nomination.

230
00:10:16,820 --> 00:10:17,820
(Laughter)

231
00:10:17,820 --> 00:10:19,820
So, you might say, OK,

232
00:10:19,820 --> 00:10:21,820
there are these differences between liberals and conservatives,

233
00:10:21,820 --> 00:10:23,820
but what makes those three other foundations moral?

234
00:10:23,820 --> 00:10:25,820
Aren't those just the foundations of xenophobia

235
00:10:25,820 --> 00:10:27,820
and authoritarianism and Puritanism?

236
00:10:27,820 --> 00:10:28,820
What makes them moral?

237
00:10:28,820 --> 00:10:31,820
The answer, I think, is contained in this incredible triptych from Hieronymus Bosch,

238
00:10:31,820 --> 00:10:33,820
"The Garden of Earthly Delights."

239
00:10:33,820 --> 00:10:36,820
In the first panel, we see the moment of creation.

240
00:10:36,820 --> 00:10:40,820
All is ordered, all is beautiful, all the people and animals

241
00:10:40,820 --> 00:10:43,820
are doing what they're supposed to be doing, where they're supposed to be.

242
00:10:43,820 --> 00:10:46,820
But then, given the way of the world, things change.

243
00:10:46,820 --> 00:10:48,820
We get every person doing whatever he wants,

244
00:10:48,820 --> 00:10:51,820
with every aperture of every other person and every other animal.

245
00:10:51,820 --> 00:10:53,820
Some of you might recognize this as the '60s.

246
00:10:53,820 --> 00:10:54,820
(Laughter)

247
00:10:54,820 --> 00:10:58,820
But the '60s inevitably gives way to the '70s,

248
00:10:58,820 --> 00:11:02,820
where the cuttings of the apertures hurt a little bit more.

249
00:11:02,820 --> 00:11:04,820
Of course, Bosch called this hell.

250
00:11:04,820 --> 00:11:07,820
So this triptych, these three panels

251
00:11:07,820 --> 00:11:12,820
portray the timeless truth that order tends to decay.

252
00:11:12,820 --> 00:11:14,820
The truth of social entropy.

253
00:11:14,820 --> 00:11:17,820
But lest you think this is just some part of the Christian imagination

254
00:11:17,820 --> 00:11:19,820
where Christians have this weird problem with pleasure,

255
00:11:19,820 --> 00:11:22,820
here's the same story, the same progression,

256
00:11:22,820 --> 00:11:25,820
told in a paper that was published in Nature a few years ago,

257
00:11:25,820 --> 00:11:29,820
in which Ernst Fehr and Simon Gachter had people play a commons dilemma.

258
00:11:29,820 --> 00:11:31,820
A game in which you give people money,

259
00:11:31,820 --> 00:11:33,820
and then, on each round of the game,

260
00:11:33,820 --> 00:11:35,820
they can put money into a common pot,

261
00:11:35,820 --> 00:11:37,820
and then the experimenter doubles what's in there,

262
00:11:37,820 --> 00:11:39,820
and then it's all divided among the players.

263
00:11:39,820 --> 00:11:42,820
So it's a really nice analog for all sorts of environmental issues,

264
00:11:42,820 --> 00:11:44,820
where we're asking people to make a sacrifice

265
00:11:44,820 --> 00:11:46,820
and they themselves don't really benefit from their own sacrifice.

266
00:11:46,820 --> 00:11:48,820
But you really want everybody else to sacrifice,

267
00:11:48,820 --> 00:11:50,820
but everybody has a temptation to a free ride.

268
00:11:50,820 --> 00:11:54,820
And what happens is that, at first, people start off reasonably cooperative --

269
00:11:54,820 --> 00:11:56,820
and this is all played anonymously.

270
00:11:56,820 --> 00:11:59,820
On the first round, people give about half of the money that they can.

271
00:11:59,820 --> 00:12:02,820
But they quickly see, "You know what, other people aren't doing so much though.

272
00:12:02,820 --> 00:12:04,820
I don't want to be a sucker. I'm not going to cooperate."

273
00:12:04,820 --> 00:12:08,820
And so cooperation quickly decays from reasonably good, down to close to zero.

274
00:12:08,820 --> 00:12:10,820
But then -- and here's the trick --

275
00:12:10,820 --> 00:12:12,820
Fehr and Gachter said, on the seventh round, they told people,

276
00:12:12,820 --> 00:12:14,820
"You know what? New rule.

277
00:12:14,820 --> 00:12:16,820
If you want to give some of your own money

278
00:12:16,820 --> 00:12:20,820
to punish people who aren't contributing, you can do that."

279
00:12:20,820 --> 00:12:23,820
And as soon as people heard about the punishment issue going on,

280
00:12:23,820 --> 00:12:25,820
cooperation shoots up.

281
00:12:25,820 --> 00:12:27,820
It shoots up and it keeps going up.

282
00:12:27,820 --> 00:12:30,820
There's a lot of research showing that to solve cooperative problems, it really helps.

283
00:12:30,820 --> 00:12:32,820
It's not enough to just appeal to people's good motives.

284
00:12:32,820 --> 00:12:34,820
It really helps to have some sort of punishment.

285
00:12:34,820 --> 00:12:36,820
Even if it's just shame or embarrassment or gossip,

286
00:12:36,820 --> 00:12:39,820
you need some sort of punishment to bring people,

287
00:12:39,820 --> 00:12:41,820
when they're in large groups, to cooperate.

288
00:12:41,820 --> 00:12:44,820
There's even some recent research suggesting that religion --

289
00:12:44,820 --> 00:12:46,820
priming God, making people think about God --

290
00:12:46,820 --> 00:12:51,820
often, in some situations, leads to more cooperative, more pro-social behavior.

291
00:12:52,820 --> 00:12:54,820
Some people think that religion is an adaptation

292
00:12:54,820 --> 00:12:56,820
evolved both by cultural and biological evolution

293
00:12:56,820 --> 00:12:58,820
to make groups to cohere,

294
00:12:58,820 --> 00:13:00,820
in part for the purpose of trusting each other,

295
00:13:00,820 --> 00:13:02,820
and then being more effective at competing with other groups.

296
00:13:02,820 --> 00:13:03,820
I think that's probably right,

297
00:13:03,820 --> 00:13:05,820
although this is a controversial issue.

298
00:13:05,820 --> 00:13:07,820
But I'm particularly interested in religion,

299
00:13:07,820 --> 00:13:10,820
and the origin of religion, and in what it does to us and for us.

300
00:13:10,820 --> 00:13:14,820
Because I think that the greatest wonder in the world is not the Grand Canyon.

301
00:13:14,820 --> 00:13:16,820
The Grand Canyon is really simple.

302
00:13:16,820 --> 00:13:19,820
It's just a lot of rock, and then a lot of water and wind, and a lot of time,

303
00:13:19,820 --> 00:13:21,820
and you get the Grand Canyon.

304
00:13:21,820 --> 00:13:22,820
It's not that complicated.

305
00:13:22,820 --> 00:13:24,820
This is what's really complicated,

306
00:13:24,820 --> 00:13:26,820
that there were people living in places like the Grand Canyon,

307
00:13:26,820 --> 00:13:28,820
cooperating with each other, or on the savannahs of Africa,

308
00:13:28,820 --> 00:13:31,820
or on the frozen shores of Alaska, and then some of these villages

309
00:13:31,820 --> 00:13:35,820
grew into the mighty cities of Babylon, and Rome, and Tenochtitlan.

310
00:13:35,820 --> 00:13:36,820
How did this happen?

311
00:13:36,820 --> 00:13:39,820
This is an absolute miracle, much harder to explain than the Grand Canyon.

312
00:13:39,820 --> 00:13:42,820
The answer, I think, is that they used every tool in the toolbox.

313
00:13:42,820 --> 00:13:44,820
It took all of our moral psychology

314
00:13:44,820 --> 00:13:46,820
to create these cooperative groups.

315
00:13:46,820 --> 00:13:48,820
Yes, you do need to be concerned about harm,

316
00:13:48,820 --> 00:13:49,820
you do need a psychology of justice.

317
00:13:49,820 --> 00:13:52,820
But it really helps to organize a group if you can have sub-groups,

318
00:13:52,820 --> 00:13:55,820
and if those sub-groups have some internal structure,

319
00:13:55,820 --> 00:13:57,820
and if you have some ideology that tells people

320
00:13:57,820 --> 00:14:01,820
to suppress their carnality, to pursue higher, nobler ends.

321
00:14:01,820 --> 00:14:03,820
And now we get to the crux of the disagreement

322
00:14:03,820 --> 00:14:05,820
between liberals and conservatives.

323
00:14:05,820 --> 00:14:07,820
Because liberals reject three of these foundations.

324
00:14:07,820 --> 00:14:10,820
They say "No, let's celebrate diversity, not common in-group membership."

325
00:14:10,820 --> 00:14:12,820
They say, "Let's question authority."

326
00:14:12,820 --> 00:14:14,820
And they say, "Keep your laws off my body."

327
00:14:14,820 --> 00:14:17,820
Liberals have very noble motives for doing this.

328
00:14:17,820 --> 00:14:20,820
Traditional authority, traditional morality can be quite repressive,

329
00:14:20,820 --> 00:14:23,820
and restrictive to those at the bottom, to women, to people that don't fit in.

330
00:14:23,820 --> 00:14:25,820
So liberals speak for the weak and oppressed.

331
00:14:25,820 --> 00:14:27,820
They want change and justice, even at the risk of chaos.

332
00:14:27,820 --> 00:14:30,820
This guy's shirt says, "Stop bitching, start a revolution."

333
00:14:30,820 --> 00:14:32,820
If you're high in openness to experience, revolution is good,

334
00:14:32,820 --> 00:14:34,820
it's change, it's fun.

335
00:14:34,820 --> 00:14:37,820
Conservatives, on the other hand, speak for institutions and traditions.

336
00:14:37,820 --> 00:14:41,820
They want order, even at some cost to those at the bottom.

337
00:14:41,820 --> 00:14:43,820
The great conservative insight is that order is really hard to achieve.

338
00:14:43,820 --> 00:14:46,820
It's really precious, and it's really easy to lose.

339
00:14:46,820 --> 00:14:48,820
So as Edmund Burke said, "The restraints on men,

340
00:14:48,820 --> 00:14:51,820
as well as their liberties, are to be reckoned among their rights."

341
00:14:51,820 --> 00:14:53,820
This was after the chaos of the French Revolution.

342
00:14:53,820 --> 00:14:55,820
So once you see this -- once you see

343
00:14:55,820 --> 00:14:58,820
that liberals and conservatives both have something to contribute,

344
00:14:58,820 --> 00:15:01,820
that they form a balance on change versus stability --

345
00:15:01,820 --> 00:15:04,820
then I think the way is open to step outside the moral matrix.

346
00:15:04,820 --> 00:15:09,820
This is the great insight that all the Asian religions have attained.

347
00:15:09,820 --> 00:15:11,820
Think about yin and yang.

348
00:15:11,820 --> 00:15:13,820
Yin and yang aren't enemies. Yin and yang don't hate each other.

349
00:15:13,820 --> 00:15:15,820
Yin and yang are both necessary, like night and day,

350
00:15:15,820 --> 00:15:17,820
for the functioning of the world.

351
00:15:17,820 --> 00:15:19,820
You find the same thing in Hinduism.

352
00:15:19,820 --> 00:15:21,820
There are many high gods in Hinduism.

353
00:15:21,820 --> 00:15:24,820
Two of them are Vishnu, the preserver, and Shiva, the destroyer.

354
00:15:24,820 --> 00:15:27,820
This image actually is both of those gods sharing the same body.

355
00:15:27,820 --> 00:15:29,820
You have the markings of Vishnu on the left,

356
00:15:29,820 --> 00:15:32,820
so we could think of Vishnu as the conservative god.

357
00:15:32,820 --> 00:15:34,820
You have the markings of Shiva on the right,

358
00:15:34,820 --> 00:15:36,820
Shiva's the liberal god. And they work together.

359
00:15:36,820 --> 00:15:38,820
You find the same thing in Buddhism.

360
00:15:38,820 --> 00:15:40,820
These two stanzas contain, I think, the deepest insights

361
00:15:40,820 --> 00:15:43,820
that have ever been attained into moral psychology.

362
00:15:43,820 --> 00:15:45,820
From the Zen master Seng-ts'an:

363
00:15:45,820 --> 00:15:49,820
"If you want the truth to stand clear before you, never be for or against.

364
00:15:49,820 --> 00:15:53,820
The struggle between for and against is the mind's worst disease."

365
00:15:53,820 --> 00:15:55,820
Now unfortunately, it's a disease

366
00:15:55,820 --> 00:15:57,820
that has been caught by many of the world's leaders.

367
00:15:57,820 --> 00:16:00,820
But before you feel superior to George Bush,

368
00:16:00,820 --> 00:16:04,820
before you throw a stone, ask yourself, do you accept this?

369
00:16:04,820 --> 00:16:07,820
Do you accept stepping out of the battle of good and evil?

370
00:16:07,820 --> 00:16:10,820
Can you be not for or against anything?

371
00:16:11,820 --> 00:16:14,820
So, what's the point? What should you do?

372
00:16:14,820 --> 00:16:16,820
Well, if you take the greatest insights

373
00:16:16,820 --> 00:16:18,820
from ancient Asian philosophies and religions,

374
00:16:18,820 --> 00:16:20,820
and you combine them with the latest research on moral psychology,

375
00:16:20,820 --> 00:16:22,820
I think you come to these conclusions:

376
00:16:22,820 --> 00:16:26,820
that our righteous minds were designed by evolution

377
00:16:26,820 --> 00:16:29,820
to unite us into teams, to divide us against other teams

378
00:16:29,820 --> 00:16:31,820
and then to blind us to the truth.

379
00:16:32,820 --> 00:16:36,820
So what should you do? Am I telling you to not strive?

380
00:16:36,820 --> 00:16:39,820
Am I telling you to embrace Seng-ts'an and stop,

381
00:16:39,820 --> 00:16:42,820
stop with this struggle of for and against?

382
00:16:42,820 --> 00:16:44,820
No, absolutely not. I'm not saying that.

383
00:16:44,820 --> 00:16:47,820
This is an amazing group of people who are doing so much,

384
00:16:47,820 --> 00:16:51,820
using so much of their talent, their brilliance, their energy, their money,

385
00:16:51,820 --> 00:16:53,820
to make the world a better place, to fight --

386
00:16:53,820 --> 00:16:56,820
to fight wrongs, to solve problems.

387
00:16:57,820 --> 00:17:01,820
But as we learned from Samantha Power, in her story

388
00:17:01,820 --> 00:17:06,820
about Sergio Vieira de Mello, you can't just go charging in,

389
00:17:06,820 --> 00:17:08,820
saying, "You're wrong, and I'm right."

390
00:17:08,820 --> 00:17:12,820
Because, as we just heard, everybody thinks they are right.

391
00:17:12,820 --> 00:17:14,820
A lot of the problems we have to solve

392
00:17:14,820 --> 00:17:17,820
are problems that require us to change other people.

393
00:17:17,820 --> 00:17:20,820
And if you want to change other people, a much better way to do it

394
00:17:20,820 --> 00:17:24,820
is to first understand who we are -- understand our moral psychology,

395
00:17:24,820 --> 00:17:27,820
understand that we all think we're right -- and then step out,

396
00:17:27,820 --> 00:17:31,820
even if it's just for a moment, step out -- check in with Seng-ts'an.

397
00:17:31,820 --> 00:17:33,820
Step out of the moral matrix,

398
00:17:33,820 --> 00:17:35,820
just try to see it as a struggle playing out,

399
00:17:35,820 --> 00:17:37,820
in which everybody does think they're right,

400
00:17:37,820 --> 00:17:39,820
and everybody, at least, has some reasons -- even if you disagree with them --

401
00:17:39,820 --> 00:17:41,820
everybody has some reasons for what they're doing.

402
00:17:41,820 --> 00:17:42,820
Step out.

403
00:17:42,820 --> 00:17:46,820
And if you do that, that's the essential move to cultivate moral humility,

404
00:17:46,820 --> 00:17:47,820
to get yourself out of this self-righteousness,

405
00:17:47,820 --> 00:17:49,820
which is the normal human condition.

406
00:17:49,820 --> 00:17:51,820
Think about the Dalai Lama.

407
00:17:51,820 --> 00:17:54,820
Think about the enormous moral authority of the Dalai Lama --

408
00:17:54,820 --> 00:17:56,820
and it comes from his moral humility.

409
00:17:58,820 --> 00:18:00,820
So I think the point -- the point of my talk,

410
00:18:00,820 --> 00:18:03,820
and I think the point of TED --

411
00:18:03,820 --> 00:18:06,820
is that this is a group that is passionately engaged

412
00:18:06,820 --> 00:18:08,820
in the pursuit of changing the world for the better.

413
00:18:08,820 --> 00:18:11,820
People here are passionately engaged

414
00:18:11,820 --> 00:18:13,820
in trying to make the world a better place.

415
00:18:13,820 --> 00:18:16,820
But there is also a passionate commitment to the truth.

416
00:18:16,820 --> 00:18:20,820
And so I think that the answer is to use that passionate commitment

417
00:18:20,820 --> 00:18:24,820
to the truth to try to turn it into a better future for us all.

418
00:18:24,820 --> 00:18:25,820
Thank you.

419
00:18:25,820 --> 00:18:28,820
(Applause)

