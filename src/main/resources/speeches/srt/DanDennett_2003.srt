1
00:00:12,820 --> 00:00:15,820
So I'm going to speak about a problem that I have

2
00:00:15,820 --> 00:00:18,820
and that's that I'm a philosopher.

3
00:00:18,820 --> 00:00:20,820
(Laughter)

4
00:00:20,820 --> 00:00:23,820
When I go to a party and people ask me what do I do

5
00:00:23,820 --> 00:00:28,820
and I say, "I'm a professor," their eyes glaze over.

6
00:00:28,820 --> 00:00:30,820
When I go to an academic cocktail party

7
00:00:30,820 --> 00:00:34,820
and there are all the professors around, they ask me what field I'm in

8
00:00:34,820 --> 00:00:37,820
and I say, "philosophy" -- their eyes glaze over.

9
00:00:37,820 --> 00:00:39,820
(Laughter)

10
00:00:39,820 --> 00:00:42,820
When I go to a philosopher's party

11
00:00:42,820 --> 00:00:45,820
(Laughter)

12
00:00:45,820 --> 00:00:49,820
and they ask me what I work on and I say, "consciousness,"

13
00:00:49,820 --> 00:00:54,820
their eyes don't glaze over -- their lips curl into a snarl.

14
00:00:54,820 --> 00:00:55,820
(Laughter)

15
00:00:55,820 --> 00:01:01,820
And I get hoots of derision and cackles and growls

16
00:01:01,820 --> 00:01:06,820
because they think, "That's impossible! You can't explain consciousness."

17
00:01:06,820 --> 00:01:08,820
The very chutzpah of somebody thinking

18
00:01:08,820 --> 00:01:12,820
that you could explain consciousness is just out of the question.

19
00:01:12,820 --> 00:01:16,820
My late, lamented friend Bob Nozick, a fine philosopher,

20
00:01:16,820 --> 00:01:20,820
in one of his books, "Philosophical Explanations,"

21
00:01:20,820 --> 00:01:25,820
is commenting on the ethos of philosophy --

22
00:01:25,820 --> 00:01:27,820
the way philosophers go about their business.

23
00:01:27,820 --> 00:01:31,820
And he says, you know, "Philosophers love rational argument."

24
00:01:31,820 --> 00:01:33,820
And he says, "It seems as if the ideal argument

25
00:01:33,820 --> 00:01:39,820
for most philosophers is you give your audience the premises

26
00:01:39,820 --> 00:01:44,820
and then you give them the inferences and the conclusion,

27
00:01:44,820 --> 00:01:48,820
and if they don't accept the conclusion, they die.

28
00:01:48,820 --> 00:01:51,820
Their heads explode." The idea is to have an argument

29
00:01:51,820 --> 00:01:55,820
that is so powerful that it knocks out your opponents.

30
00:01:55,820 --> 00:01:58,820
But in fact that doesn't change people's minds at all.

31
00:01:58,820 --> 00:01:59,820
It's very hard to change people's minds

32
00:01:59,820 --> 00:02:01,820
about something like consciousness,

33
00:02:01,820 --> 00:02:06,820
and I finally figured out the reason for that.

34
00:02:06,820 --> 00:02:10,820
The reason for that is that everybody's an expert on consciousness.

35
00:02:10,820 --> 00:02:14,820
We heard the other day that everybody's got a strong opinion about video games.

36
00:02:14,820 --> 00:02:17,820
They all have an idea for a video game, even if they're not experts.

37
00:02:17,820 --> 00:02:20,820
But they don't consider themselves experts on video games;

38
00:02:20,820 --> 00:02:21,820
they've just got strong opinions.

39
00:02:21,820 --> 00:02:26,820
I'm sure that people here who work on, say, climate change

40
00:02:26,820 --> 00:02:31,820
and global warming, or on the future of the Internet,

41
00:02:31,820 --> 00:02:33,820
encounter people who have very strong opinions

42
00:02:33,820 --> 00:02:36,820
about what's going to happen next.

43
00:02:36,820 --> 00:02:40,820
But they probably don't think of these opinions as expertise.

44
00:02:40,820 --> 00:02:42,820
They're just strongly held opinions.

45
00:02:42,820 --> 00:02:46,820
But with regard to consciousness, people seem to think,

46
00:02:46,820 --> 00:02:49,820
each of us seems to think, "I am an expert.

47
00:02:49,820 --> 00:02:52,820
Simply by being conscious, I know all about this."

48
00:02:52,820 --> 00:02:54,820
And so, you tell them your theory and they say,

49
00:02:54,820 --> 00:02:55,820
"No, no, that's not the way consciousness is!

50
00:02:55,820 --> 00:02:57,820
No, you've got it all wrong."

51
00:02:57,820 --> 00:03:01,820
And they say this with an amazing confidence.

52
00:03:01,820 --> 00:03:03,820
And so what I'm going to try to do today

53
00:03:03,820 --> 00:03:06,820
is to shake your confidence. Because I know the feeling --

54
00:03:06,820 --> 00:03:08,820
I can feel it myself.

55
00:03:08,820 --> 00:03:14,820
I want to shake your confidence that you know your own innermost minds --

56
00:03:14,820 --> 00:03:19,820
that you are, yourselves, authoritative about your own consciousness.

57
00:03:19,820 --> 00:03:22,820
That's the order of the day here.

58
00:03:22,820 --> 00:03:25,820
Now, this nice picture shows a thought-balloon, a thought-bubble.

59
00:03:25,820 --> 00:03:27,820
I think everybody understands what that means.

60
00:03:27,820 --> 00:03:30,820
That's supposed to exhibit the stream of consciousness.

61
00:03:30,820 --> 00:03:32,820
This is my favorite picture of consciousness that's ever been done.

62
00:03:32,820 --> 00:03:35,820
It's a Saul Steinberg of course -- it was a New Yorker cover.

63
00:03:35,820 --> 00:03:40,820
And this fellow here is looking at the painting by Braque.

64
00:03:40,820 --> 00:03:44,820
That reminds him of the word baroque, barrack, bark, poodle,

65
00:03:44,820 --> 00:03:46,820
Suzanne R. -- he's off to the races.

66
00:03:46,820 --> 00:03:50,820
There's a wonderful stream of consciousness here

67
00:03:50,820 --> 00:03:54,820
and if you follow it along, you learn a lot about this man.

68
00:03:54,820 --> 00:03:56,820
What I particularly like about this picture, too,

69
00:03:56,820 --> 00:03:58,820
is that Steinberg has rendered the guy

70
00:03:58,820 --> 00:04:01,820
in this sort of pointillist style.

71
00:04:01,820 --> 00:04:04,820
Which reminds us, as Rod Brooks was saying yesterday:

72
00:04:04,820 --> 00:04:08,820
what we are, what each of us is -- what you are, what I am --

73
00:04:08,820 --> 00:04:14,820
is approximately 100 trillion little cellular robots.

74
00:04:14,820 --> 00:04:16,820
That's what we're made of.

75
00:04:16,820 --> 00:04:20,820
No other ingredients at all. We're just made of cells, about 100 trillion of them.

76
00:04:20,820 --> 00:04:22,820
Not a single one of those cells is conscious;

77
00:04:22,820 --> 00:04:27,820
not a single one of those cells knows who you are, or cares.

78
00:04:27,820 --> 00:04:29,820
Somehow, we have to explain

79
00:04:29,820 --> 00:04:33,820
how when you put together teams, armies, battalions

80
00:04:33,820 --> 00:04:37,820
of hundreds of millions of little robotic unconscious cells --

81
00:04:37,820 --> 00:04:41,820
not so different really from a bacterium, each one of them --

82
00:04:41,820 --> 00:04:45,820
the result is this. I mean, just look at it.

83
00:04:45,820 --> 00:04:49,820
The content -- there's color, there's ideas, there's memories,

84
00:04:49,820 --> 00:04:53,820
there's history. And somehow all that content of consciousness

85
00:04:53,820 --> 00:04:58,820
is accomplished by the busy activity of those hoards of neurons.

86
00:04:58,820 --> 00:05:02,820
How is that possible? Many people just think it isn't possible at all.

87
00:05:02,820 --> 00:05:04,820
They think, "No, there can't be any

88
00:05:04,820 --> 00:05:08,820
sort of naturalistic explanation of consciousness."

89
00:05:08,820 --> 00:05:11,820
This is a lovely book by a friend of mine named Lee Siegel,

90
00:05:11,820 --> 00:05:14,820
who's a professor of religion, actually, at the University of Hawaii,

91
00:05:14,820 --> 00:05:16,820
and he's an expert magician, and an expert

92
00:05:16,820 --> 00:05:20,820
on the street magic of India, which is what this book is about,

93
00:05:20,820 --> 00:05:22,820
"Net of Magic."

94
00:05:22,820 --> 00:05:25,820
And there's a passage in it which I would love to share with you.

95
00:05:25,820 --> 00:05:31,820
It speaks so eloquently to the problem.

96
00:05:31,820 --> 00:05:36,820
"'I'm writing a book on magic,' I explain, and I'm asked, 'Real magic?'

97
00:05:36,820 --> 00:05:38,820
By 'real magic,' people mean miracles,

98
00:05:38,820 --> 00:05:40,820
thaumaturgical acts, and supernatural powers.

99
00:05:40,820 --> 00:05:44,820
'No,' I answer. 'Conjuring tricks, not real magic.'

100
00:05:44,820 --> 00:05:48,820
'Real magic,' in other words, refers to the magic that is not real;

101
00:05:48,820 --> 00:05:53,820
while the magic that is real, that can actually be done, is not real magic."

102
00:05:53,820 --> 00:05:57,820
(Laughter)

103
00:05:57,820 --> 00:06:01,820
Now, that's the way a lot of people feel about consciousness.

104
00:06:01,820 --> 00:06:02,820
(Laughter)

105
00:06:02,820 --> 00:06:04,820
Real consciousness is not a bag of tricks.

106
00:06:04,820 --> 00:06:06,820
If you're going to explain this as a bag of tricks,

107
00:06:06,820 --> 00:06:09,820
then it's not real consciousness, whatever it is.

108
00:06:09,820 --> 00:06:15,820
And, as Marvin said, and as other people have said,

109
00:06:15,820 --> 00:06:18,820
"Consciousness is a bag of tricks."

110
00:06:18,820 --> 00:06:23,820
This means that a lot of people are just left completely dissatisfied

111
00:06:23,820 --> 00:06:26,820
and incredulous when I attempt to explain consciousness.

112
00:06:26,820 --> 00:06:29,820
So this is the problem. So I have to

113
00:06:29,820 --> 00:06:32,820
do a little bit of the sort of work

114
00:06:32,820 --> 00:06:36,820
that a lot of you won't like,

115
00:06:36,820 --> 00:06:38,820
for the same reason that you don't like to see

116
00:06:38,820 --> 00:06:40,820
a magic trick explained to you.

117
00:06:40,820 --> 00:06:44,820
How many of you here, if somebody -- some smart aleck --

118
00:06:44,820 --> 00:06:47,820
starts telling you how a particular magic trick is done,

119
00:06:47,820 --> 00:06:50,820
you sort of want to block your ears and say, "No, no, I don't want to know!

120
00:06:50,820 --> 00:06:53,820
Don't take the thrill of it away. I'd rather be mystified.

121
00:06:53,820 --> 00:06:56,820
Don't tell me the answer."

122
00:06:56,820 --> 00:06:59,820
A lot of people feel that way about consciousness, I've discovered.

123
00:06:59,820 --> 00:07:05,820
And I'm sorry if I impose some clarity, some understanding on you.

124
00:07:05,820 --> 00:07:10,820
You'd better leave now if you don't want to know some of these tricks.

125
00:07:10,820 --> 00:07:14,820
But I'm not going to explain it all to you.

126
00:07:14,820 --> 00:07:17,820
I'm going to do what philosophers do.

127
00:07:17,820 --> 00:07:23,820
Here's how a philosopher explains the sawing-the-lady-in-half trick.

128
00:07:23,820 --> 00:07:25,820
You know the sawing-the-lady-in-half trick?

129
00:07:25,820 --> 00:07:29,820
The philosopher says, "I'm going to explain to you how that's done.

130
00:07:29,820 --> 00:07:34,820
You see, the magician doesn't really saw the lady in half."

131
00:07:34,820 --> 00:07:36,820
(Laughter)

132
00:07:36,820 --> 00:07:40,820
"He merely makes you think that he does."

133
00:07:40,820 --> 00:07:41,820
And you say, "Yes, and how does he do that?"

134
00:07:41,820 --> 00:07:43,820
He says, "Oh, that's not my department, I'm sorry."

135
00:07:43,820 --> 00:07:48,820
(Laughter)

136
00:07:48,820 --> 00:07:51,820
So now I'm going to illustrate how philosophers explain consciousness.

137
00:07:51,820 --> 00:07:54,820
But I'm going to try to also show you

138
00:07:54,820 --> 00:07:57,820
that consciousness isn't quite as marvelous --

139
00:07:57,820 --> 00:07:59,820
your own consciousness isn't quite as wonderful --

140
00:07:59,820 --> 00:08:01,820
as you may have thought it is.

141
00:08:01,820 --> 00:08:05,820
This is something, by the way, that Lee Siegel talks about in his book.

142
00:08:05,820 --> 00:08:09,820
He marvels at how he'll do a magic show, and afterwards

143
00:08:09,820 --> 00:08:13,820
people will swear they saw him do X, Y, and Z. He never did those things.

144
00:08:13,820 --> 00:08:15,820
He didn't even try to do those things.

145
00:08:15,820 --> 00:08:19,820
People's memories inflate what they think they saw.

146
00:08:19,820 --> 00:08:22,820
And the same is true of consciousness.

147
00:08:22,820 --> 00:08:29,820
Now, let's see if this will work. All right. Let's just watch this.

148
00:08:29,820 --> 00:08:30,820
Watch it carefully.

149
00:08:42,820 --> 00:08:45,820
I'm working with a young computer-animator documentarian

150
00:08:45,820 --> 00:08:50,820
named Nick Deamer, and this is a little demo that he's done for me,

151
00:08:50,820 --> 00:08:53,820
part of a larger project some of you may be interested in.

152
00:08:53,820 --> 00:08:56,820
We're looking for a backer.

153
00:08:56,820 --> 00:09:00,820
It's a feature-length documentary on consciousness.

154
00:09:00,820 --> 00:09:02,820
OK, now, you all saw what changed, right?

155
00:09:06,820 --> 00:09:11,820
How many of you noticed that every one of those squares changed color?

156
00:09:11,820 --> 00:09:15,820
Every one. I'll just show you by running it again.

157
00:09:20,820 --> 00:09:25,820
Even when you know that they're all going to change color,

158
00:09:25,820 --> 00:09:29,820
it's very hard to notice. You have to really concentrate

159
00:09:29,820 --> 00:09:32,820
to pick up any of the changes at all.

160
00:09:32,820 --> 00:09:37,820
Now, this is an example -- one of many --

161
00:09:37,820 --> 00:09:39,820
of a phenomenon that's now being studied quite a bit.

162
00:09:39,820 --> 00:09:43,820
It's one that I predicted in the last page or two of my

163
00:09:43,820 --> 00:09:45,820
1991 book, "Consciousness Explained,"

164
00:09:45,820 --> 00:09:48,820
where I said if you did experiments of this sort,

165
00:09:48,820 --> 00:09:51,820
you'd find that people were unable to pick up really large changes.

166
00:09:51,820 --> 00:09:53,820
If there's time at the end,

167
00:09:53,820 --> 00:09:56,820
I'll show you the much more dramatic case.

168
00:09:56,820 --> 00:10:01,820
Now, how can it be that there are all those changes going on,

169
00:10:01,820 --> 00:10:04,820
and that we're not aware of them?

170
00:10:04,820 --> 00:10:09,820
Well, earlier today, Jeff Hawkins mentioned the way your eye saccades,

171
00:10:09,820 --> 00:10:12,820
the way your eye moves around three or four times a second.

172
00:10:12,820 --> 00:10:15,820
He didn't mention the speed. Your eye is constantly in motion,

173
00:10:15,820 --> 00:10:18,820
moving around, looking at eyes, noses, elbows,

174
00:10:18,820 --> 00:10:20,820
looking at interesting things in the world.

175
00:10:20,820 --> 00:10:22,820
And where your eye isn't looking,

176
00:10:22,820 --> 00:10:25,820
you're remarkably impoverished in your vision.

177
00:10:25,820 --> 00:10:28,820
That's because the foveal part of your eye,

178
00:10:28,820 --> 00:10:30,820
which is the high-resolution part,

179
00:10:30,820 --> 00:10:33,820
is only about the size of your thumbnail held at arms length.

180
00:10:33,820 --> 00:10:35,820
That's the detail part.

181
00:10:35,820 --> 00:10:38,820
It doesn't seem that way, does it?

182
00:10:38,820 --> 00:10:40,820
It doesn't seem that way, but that's the way it is.

183
00:10:40,820 --> 00:10:44,820
You're getting in a lot less information than you think.

184
00:10:44,820 --> 00:10:50,820
Here's a completely different effect. This is a painting by Bellotto.

185
00:10:50,820 --> 00:10:52,820
It's in the museum in North Carolina.

186
00:10:52,820 --> 00:10:55,820
Bellotto was a student of Canaletto's.

187
00:10:55,820 --> 00:10:56,820
And I love paintings like that --

188
00:10:56,820 --> 00:11:00,820
the painting is actually about as big as it is right here.

189
00:11:00,820 --> 00:11:03,820
And I love Canalettos, because Canaletto has this fantastic detail,

190
00:11:03,820 --> 00:11:06,820
and you can get right up

191
00:11:06,820 --> 00:11:09,820
and see all the details on the painting.

192
00:11:09,820 --> 00:11:14,820
And I started across the hall in North Carolina,

193
00:11:14,820 --> 00:11:16,820
because I thought it was probably a Canaletto,

194
00:11:16,820 --> 00:11:18,820
and would have all that in detail.

195
00:11:18,820 --> 00:11:21,820
And I noticed that on the bridge there, there's a lot of people --

196
00:11:21,820 --> 00:11:24,820
you can just barely see them walking across the bridge.

197
00:11:24,820 --> 00:11:25,820
And I thought as I got closer

198
00:11:25,820 --> 00:11:28,820
I would be able to see all the detail of most people,

199
00:11:28,820 --> 00:11:30,820
see their clothes, and so forth.

200
00:11:30,820 --> 00:11:34,820
And as I got closer and closer, I actually screamed.

201
00:11:34,820 --> 00:11:36,820
I yelled out because when I got closer,

202
00:11:36,820 --> 00:11:40,820
I found the detail wasn't there at all.

203
00:11:40,820 --> 00:11:44,820
There were just little artfully placed blobs of paint.

204
00:11:44,820 --> 00:11:47,820
And as I walked towards the picture,

205
00:11:47,820 --> 00:11:50,820
I was expecting detail that wasn't there.

206
00:11:50,820 --> 00:11:55,820
The artist had very cleverly suggested people and clothes

207
00:11:55,820 --> 00:11:58,820
and wagons and all sorts of things,

208
00:11:58,820 --> 00:12:01,820
and my brain had taken the suggestion.

209
00:12:01,820 --> 00:12:07,820
You're familiar with a more recent technology, which is -- There,

210
00:12:07,820 --> 00:12:09,820
you can get a better view of the blobs.

211
00:12:09,820 --> 00:12:11,820
See, when you get close

212
00:12:11,820 --> 00:12:16,820
they're really just blobs of paint.

213
00:12:16,820 --> 00:12:22,820
You will have seen something like this -- this is the reverse effect.

214
00:12:30,820 --> 00:12:33,820
I'll just give that to you one more time.

215
00:12:33,820 --> 00:12:40,820
Now, what does your brain do when it takes the suggestion?

216
00:12:40,820 --> 00:12:45,820
When an artful blob of paint or two, by an artist,

217
00:12:45,820 --> 00:12:51,820
suggests a person -- say, one of

218
00:12:51,820 --> 00:12:53,820
Marvin Minsky's little society of mind --

219
00:12:53,820 --> 00:12:58,820
do they send little painters out to fill in all the details in your brain somewhere?

220
00:12:58,820 --> 00:13:03,820
I don't think so. Not a chance. But then, how on Earth is it done?

221
00:13:03,820 --> 00:13:08,820
Well, remember the philosopher's explanation of the lady?

222
00:13:08,820 --> 00:13:11,820
It's the same thing.

223
00:13:11,820 --> 00:13:14,820
The brain just makes you think that it's got the detail there.

224
00:13:14,820 --> 00:13:17,820
You think the detail's there, but it isn't there.

225
00:13:17,820 --> 00:13:20,820
The brain isn't actually putting the detail in your head at all.

226
00:13:20,820 --> 00:13:23,820
It's just making you expect the detail.

227
00:13:23,820 --> 00:13:26,820
Let's just do this experiment very quickly.

228
00:13:26,820 --> 00:13:31,820
Is the shape on the left the same as the shape on the right, rotated?

229
00:13:31,820 --> 00:13:33,820
Yes.

230
00:13:33,820 --> 00:13:35,820
How many of you did it by rotating the one on the left

231
00:13:35,820 --> 00:13:38,820
in your mind's eye, to see if it matched up with the one on the right?

232
00:13:38,820 --> 00:13:42,820
How many of you rotated the one on the right? OK.

233
00:13:42,820 --> 00:13:44,820
How do you know that's what you did?

234
00:13:44,820 --> 00:13:47,820
(Laughter)

235
00:13:47,820 --> 00:13:49,820
There's in fact been a very interesting debate

236
00:13:49,820 --> 00:13:52,820
raging for over 20 years in cognitive science --

237
00:13:52,820 --> 00:13:54,820
various experiments started by Roger Shepherd,

238
00:13:54,820 --> 00:13:59,820
who measured the angular velocity of rotation of mental images.

239
00:13:59,820 --> 00:14:01,820
Yes, it's possible to do that.

240
00:14:01,820 --> 00:14:08,820
But the details of the process are still in significant controversy.

241
00:14:08,820 --> 00:14:11,820
And if you read that literature, one of the things

242
00:14:11,820 --> 00:14:14,820
that you really have to come to terms with is

243
00:14:14,820 --> 00:14:16,820
even when you're the subject in the experiment, you don't know.

244
00:14:16,820 --> 00:14:18,820
You don't know how you do it.

245
00:14:18,820 --> 00:14:21,820
You just know that you have certain beliefs.

246
00:14:21,820 --> 00:14:24,820
And they come in a certain order, at a certain time.

247
00:14:24,820 --> 00:14:26,820
And what explains the fact that that's what you think?

248
00:14:26,820 --> 00:14:30,820
Well, that's where you have to go backstage and ask the magician.

249
00:14:30,820 --> 00:14:34,820
This is a figure that I love: Bradley, Petrie, and Dumais.

250
00:14:34,820 --> 00:14:36,820
You may think that I've cheated,

251
00:14:36,820 --> 00:14:41,820
that I've put a little whiter-than-white boundary there.

252
00:14:41,820 --> 00:14:43,820
How many of you see that sort of boundary,

253
00:14:43,820 --> 00:14:46,820
with the Necker cube floating in front of the circles?

254
00:14:46,820 --> 00:14:48,820
Can you see it?

255
00:14:48,820 --> 00:14:53,820
Well, you know, in effect, the boundary's really there, in a certain sense.

256
00:14:53,820 --> 00:14:56,820
Your brain is actually computing that boundary,

257
00:14:56,820 --> 00:15:01,820
the boundary that goes right there.

258
00:15:01,820 --> 00:15:03,820
But now, notice there are two ways of seeing the cube, right?

259
00:15:03,820 --> 00:15:05,820
It's a Necker cube.

260
00:15:05,820 --> 00:15:09,820
Everybody can see the two ways of seeing the cube? OK.

261
00:15:09,820 --> 00:15:13,820
Can you see the four ways of seeing the cube?

262
00:15:13,820 --> 00:15:15,820
Because there's another way of seeing it.

263
00:15:15,820 --> 00:15:18,820
If you're seeing it as a cube floating in front of some circles,

264
00:15:18,820 --> 00:15:21,820
some black circles, there's another way of seeing it.

265
00:15:21,820 --> 00:15:23,820
As a cube, on a black background,

266
00:15:23,820 --> 00:15:25,820
as seen through a piece of Swiss cheese.

267
00:15:25,820 --> 00:15:28,820
(Laughter)

268
00:15:28,820 --> 00:15:34,820
Can you get it? How many of you can't get it? That'll help.

269
00:15:34,820 --> 00:15:36,820
(Laughter)

270
00:15:36,820 --> 00:15:41,820
Now you can get it. These are two very different phenomena.

271
00:15:41,820 --> 00:15:47,820
When you see the cube one way, behind the screen,

272
00:15:47,820 --> 00:15:49,820
those boundaries go away.

273
00:15:49,820 --> 00:15:54,820
But there's still a sort of filling in, as we can tell if we look at this.

274
00:15:54,820 --> 00:15:58,820
We don't have any trouble seeing the cube, but where does the color change?

275
00:15:58,820 --> 00:16:01,820
Does your brain have to send little painters in there?

276
00:16:01,820 --> 00:16:03,820
The purple-painters and the green-painters

277
00:16:03,820 --> 00:16:06,820
fight over who's going to paint that bit behind the curtain? No.

278
00:16:06,820 --> 00:16:10,820
Your brain just lets it go. The brain doesn't need to fill that in.

279
00:16:15,820 --> 00:16:18,820
When I first started talking about

280
00:16:18,820 --> 00:16:22,820
the Bradley, Petrie, Dumais example that you just saw --

281
00:16:22,820 --> 00:16:26,820
I'll go back to it, this one --

282
00:16:26,820 --> 00:16:33,820
I said that there was no filling-in behind there.

283
00:16:33,820 --> 00:16:36,820
And I supposed that that was just a flat truth, always true.

284
00:16:36,820 --> 00:16:41,820
But Rob Van Lier has recently shown that it isn't.

285
00:16:41,820 --> 00:16:46,820
Now, if you think you see some pale yellow --

286
00:16:46,820 --> 00:16:48,820
I'll run this a few more times.

287
00:16:48,820 --> 00:16:52,820
Look in the gray areas,

288
00:16:52,820 --> 00:16:57,820
and see if you seem to see something sort of shadowy moving in there --

289
00:16:57,820 --> 00:17:04,820
yeah, it's amazing. There's nothing there. It's no trick.

290
00:17:04,820 --> 00:17:10,820
["Failure to Detect Changes in Scenes" slide]

291
00:17:10,820 --> 00:17:12,820
This is Ron Rensink's work, which was in some degree

292
00:17:12,820 --> 00:17:16,820
inspired by that suggestion right at the end of the book.

293
00:17:16,820 --> 00:17:18,820
Let me just pause this for a second if I can.

294
00:17:18,820 --> 00:17:20,820
This is change-blindness.

295
00:17:20,820 --> 00:17:22,820
What you're going to see is two pictures,

296
00:17:22,820 --> 00:17:24,820
one of which is slightly different from the other.

297
00:17:24,820 --> 00:17:27,820
You see here the red roof and the gray roof,

298
00:17:27,820 --> 00:17:29,820
and in between them there will be a mask,

299
00:17:29,820 --> 00:17:33,820
which is just a blank screen, for about a quarter of a second.

300
00:17:33,820 --> 00:17:35,820
So you'll see the first picture, then a mask,

301
00:17:35,820 --> 00:17:37,820
then the second picture, then a mask.

302
00:17:37,820 --> 00:17:41,820
And this will just continue, and your job as the subject

303
00:17:41,820 --> 00:17:44,820
is to press the button when you see the change.

304
00:17:44,820 --> 00:17:52,820
So, show the original picture for 240 milliseconds. Blank.

305
00:17:52,820 --> 00:17:58,820
Show the next picture for 240 milliseconds. Blank.

306
00:17:58,820 --> 00:18:02,820
And keep going, until the subject presses the button, saying,

307
00:18:02,820 --> 00:18:04,820
"I see the change."

308
00:18:04,820 --> 00:18:07,820
So now we're going to be subjects in the experiment.

309
00:18:07,820 --> 00:18:16,820
We're going to start easy. Some examples.

310
00:18:16,820 --> 00:18:18,820
No trouble there.

311
00:18:18,820 --> 00:18:21,820
Can everybody see? All right.

312
00:18:21,820 --> 00:18:25,820
Indeed, Rensink's subjects took only a little bit more

313
00:18:25,820 --> 00:18:27,820
than a second to press the button.

314
00:18:32,820 --> 00:18:33,820
Can you see that one?

315
00:18:41,820 --> 00:18:43,820
2.9 seconds.

316
00:18:50,820 --> 00:18:53,820
How many don't see it still?

317
00:18:53,820 --> 00:18:55,820
What's on the roof of that barn?

318
00:18:55,820 --> 00:18:58,820
(Laughter)

319
00:19:06,820 --> 00:19:08,820
It's easy.

320
00:19:32,820 --> 00:19:34,820
Is it a bridge or a dock?

321
00:19:38,820 --> 00:19:42,820
There are a few more really dramatic ones, and then I'll close.

322
00:19:42,820 --> 00:19:46,820
I want you to see a few that are particularly striking.

323
00:19:46,820 --> 00:19:53,820
This one because it's so large and yet it's pretty hard to see.

324
00:19:53,820 --> 00:19:56,820
Can you see it?

325
00:19:56,820 --> 00:19:58,820
Audience: Yes.

326
00:19:58,820 --> 00:20:01,820
Dan Dennett: See the shadows going back and forth? Pretty big.

327
00:20:09,820 --> 00:20:13,820
So 15.5 seconds is the median time

328
00:20:13,820 --> 00:20:15,820
for subjects in his experiment there.

329
00:20:15,820 --> 00:20:18,820
I love this one. I'll end with this one,

330
00:20:18,820 --> 00:20:21,820
just because it's such an obvious and important thing.

331
00:20:23,820 --> 00:20:29,820
How many still don't see it? How many still don't see it?

332
00:20:29,820 --> 00:20:32,820
How many engines on the wing of that Boeing?

333
00:20:32,820 --> 00:20:33,820
(Laughter)

334
00:20:33,820 --> 00:20:39,820
Right in the middle of the picture!

335
00:20:39,820 --> 00:20:40,820
Thanks very much for your attention.

336
00:20:40,820 --> 00:20:45,820
What I wanted to show you is that scientists,

337
00:20:45,820 --> 00:20:49,820
using their from-the-outside, third-person methods,

338
00:20:49,820 --> 00:20:51,820
can tell you things about your own consciousness

339
00:20:51,820 --> 00:20:53,820
that you would never dream of,

340
00:20:53,820 --> 00:20:55,820
and that, in fact, you're not the authority

341
00:20:55,820 --> 00:20:57,820
on your own consciousness that you think you are.

342
00:20:57,820 --> 00:20:59,820
And we're really making a lot of progress

343
00:20:59,820 --> 00:21:02,820
on coming up with a theory of mind.

344
00:21:02,820 --> 00:21:08,820
Jeff Hawkins, this morning, was describing his attempt

345
00:21:08,820 --> 00:21:12,820
to get theory, and a good, big theory, into the neuroscience.

346
00:21:12,820 --> 00:21:17,820
And he's right. This is a problem.

347
00:21:17,820 --> 00:21:19,820
Harvard Medical School once -- I was at a talk --

348
00:21:19,820 --> 00:21:23,820
director of the lab said, "In our lab, we have a saying.

349
00:21:23,820 --> 00:21:26,820
If you work on one neuron, that's neuroscience.

350
00:21:26,820 --> 00:21:29,820
If you work on two neurons, that's psychology."

351
00:21:29,820 --> 00:21:33,820
(Laughter)

352
00:21:33,820 --> 00:21:36,820
We have to have more theory, and it can come as much from the top down.

353
00:21:36,820 --> 00:21:38,820
Thank you very much.

354
00:21:38,820 --> 00:21:42,820
(Applause)

